{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPrsMwbj/31HwlLu5vqtekI",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/2302A52219/Generative-AI/blob/main/Ass_9_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BAhtTZaElMCA",
        "outputId": "13ce9266-51c7-4426-fbe4-7cb41e9cb2af"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "\u001b[1m11490434/11490434\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/reshaping/flatten.py:37: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 6ms/step - accuracy: 0.1259 - loss: 2.3140 - val_accuracy: 0.1465 - val_loss: 2.3049\n",
            "Epoch 2/10\n",
            "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.1515 - loss: 2.2998 - val_accuracy: 0.1765 - val_loss: 2.2910\n",
            "Epoch 3/10\n",
            "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - accuracy: 0.1806 - loss: 2.2875 - val_accuracy: 0.2062 - val_loss: 2.2772\n",
            "Epoch 4/10\n",
            "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5ms/step - accuracy: 0.2045 - loss: 2.2737 - val_accuracy: 0.2235 - val_loss: 2.2633\n",
            "Epoch 5/10\n",
            "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.2231 - loss: 2.2609 - val_accuracy: 0.2400 - val_loss: 2.2494\n",
            "Epoch 6/10\n",
            "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.2346 - loss: 2.2480 - val_accuracy: 0.2541 - val_loss: 2.2357\n",
            "Epoch 7/10\n",
            "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.2515 - loss: 2.2345 - val_accuracy: 0.2686 - val_loss: 2.2221\n",
            "Epoch 8/10\n",
            "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.2605 - loss: 2.2228 - val_accuracy: 0.2824 - val_loss: 2.2084\n",
            "Epoch 9/10\n",
            "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.2759 - loss: 2.2082 - val_accuracy: 0.2973 - val_loss: 2.1946\n",
            "Epoch 10/10\n",
            "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.2891 - loss: 2.1944 - val_accuracy: 0.3113 - val_loss: 2.1804\n",
            "Training Accuracy: 0.2995\n",
            "Testing Accuracy: 0.3113\n",
            "Best Architecture: {'layers': [128, 128, 128], 'activation': 'tanh'}\n",
            "Best Testing Accuracy: 0.7486\n"
          ]
        }
      ],
      "source": [
        "# Import necessary libraries\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Flatten\n",
        "from tensorflow.keras.optimizers import Adadelta\n",
        "from tensorflow.keras.datasets import mnist\n",
        "import numpy as np\n",
        "\n",
        "# Load the MNIST dataset\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "\n",
        "# Normalize the data (scaling pixel values to range [0,1])\n",
        "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
        "\n",
        "# Define the ANN Model Architecture\n",
        "def build_model(hidden_layers=[32, 32, 32], activation='relu'):\n",
        "    model = Sequential()\n",
        "    model.add(Flatten(input_shape=(28, 28)))  # Flatten input image (28x28) to a vector\n",
        "\n",
        "    # Add hidden layers\n",
        "    for neurons in hidden_layers:\n",
        "        model.add(Dense(neurons, activation=activation))\n",
        "\n",
        "    model.add(Dense(10, activation='softmax'))  # Output layer for 10 digit classes\n",
        "\n",
        "    # Compile the model\n",
        "    model.compile(optimizer=Adadelta(),\n",
        "                  loss='sparse_categorical_crossentropy',\n",
        "                  metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "# Train and evaluate the default model\n",
        "model = build_model()\n",
        "history = model.fit(x_train, y_train, epochs=10, batch_size=128, validation_data=(x_test, y_test), verbose=1)\n",
        "\n",
        "# Evaluate performance\n",
        "train_loss, train_acc = model.evaluate(x_train, y_train, verbose=0)\n",
        "test_loss, test_acc = model.evaluate(x_test, y_test, verbose=0)\n",
        "\n",
        "# Print accuracy results\n",
        "print(f\"Training Accuracy: {train_acc:.4f}\")\n",
        "print(f\"Testing Accuracy: {test_acc:.4f}\")\n",
        "\n",
        "# Hyperparameter tuning (experimenting with different architectures)\n",
        "tuned_models = [\n",
        "    {\"layers\": [64, 64], \"activation\": \"relu\"},\n",
        "    {\"layers\": [128, 64, 32], \"activation\": \"relu\"},\n",
        "    {\"layers\": [256, 128, 64], \"activation\": \"relu\"},\n",
        "    {\"layers\": [128, 128, 128], \"activation\": \"tanh\"},\n",
        "]\n",
        "\n",
        "best_acc = 0\n",
        "best_model_config = None\n",
        "\n",
        "# Train and evaluate each model\n",
        "for config in tuned_models:\n",
        "    model = build_model(hidden_layers=config[\"layers\"], activation=config[\"activation\"])\n",
        "    model.fit(x_train, y_train, epochs=10, batch_size=128, validation_data=(x_test, y_test), verbose=0)\n",
        "\n",
        "    _, test_acc = model.evaluate(x_test, y_test, verbose=0)\n",
        "\n",
        "    if test_acc > best_acc:\n",
        "        best_acc = test_acc\n",
        "        best_model_config = config\n",
        "\n",
        "# Print the best model configuration\n",
        "print(\"Best Architecture:\", best_model_config)\n",
        "print(f\"Best Testing Accuracy: {best_acc:.4f}\")"
      ]
    }
  ]
}